{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIA READINGROOM > CREST > SCIENTIFIC ABSTRACTS \n",
    "\n",
    "Scraping tests for metadata + pdf retrieval (pre-OCR tasks)<br>10.17.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from os.path import expanduser\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib2\n",
    "import time\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASEURL = \"https://www.cia.gov\"\n",
    "PAGINATE_PATH =\"/library/readingroom/collection/scientific-abstracts?page=\"\n",
    "PDF_DIR = expanduser(\"~\") + \"/cia_pdfs/\"\n",
    "RANGE = 1654 # pages, of 20 docs per page\n",
    "TEST_RANGE = 10\n",
    "SKIPPED_FILES = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_file(url, name):\n",
    "    filepath = PDF_DIR + name + \".pdf\"\n",
    "    if not os.path.exists(PDF_DIR):\n",
    "        os.makedirs(PDF_DIR)\n",
    "    try:\n",
    "        response = urllib2.urlopen(url)\n",
    "        with open(filepath, 'w+') as f:\n",
    "            f.write(response.read())\n",
    "            f.close()\n",
    "    except urllib2.URLError as e:\n",
    "        print ('WiFi connection perhaps lost !! Trying one more time...')\n",
    "        try:\n",
    "            response = urllib2.urlopen(url)\n",
    "            with open(filepath, 'w+') as f:\n",
    "                f.write(response.read())\n",
    "                f.close()\n",
    "        except:\n",
    "            print ('WiFi connection really lost !! Bailing out..')\n",
    "            print (e)\n",
    "            SKIPPED_FILES.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sciab_df(download_bool):\n",
    "    \n",
    "    df = pd.DataFrame(columns=['id','title','classification','publication_date'])\n",
    "    idx = 0\n",
    "    \n",
    "    for i in tqdm(xrange(TEST_RANGE)):\n",
    "        \n",
    "        pagination_link = BASEURL + PAGINATE_PATH + str(i)\n",
    "        pagination_page = requests.get(pagination_link)\n",
    "        p_soup = BeautifulSoup(pagination_page.content, 'lxml')\n",
    "        \n",
    "        for doc_title in p_soup.find_all(\"h4\", class_=\"field-content\"):\n",
    "            \n",
    "            a = doc_title.select_one(\"a\")\n",
    "            link = str(a.get('href'))\n",
    "            TITLE = str(a.string or \"\")\n",
    "            \n",
    "            doc_page = requests.get(BASEURL + link)\n",
    "            m_soup = BeautifulSoup(doc_page.content, 'lxml')\n",
    "            time.sleep (.05)\n",
    "        \n",
    "            try:\n",
    "                PUB_DATE = str(m_soup.select_one(\".field-name-field-pub-date\").select_one(\"span\").get('content') or \"\")\n",
    "            except:\n",
    "                print \"Invalid PUB_DATE field. Continuing.\"\n",
    "                SKIPPED_FILES.append(name)\n",
    "            try:\n",
    "                ID = str(m_soup.select_one(\".field-name-field-document-number\").select_one(\".field-item\").string or \"\")\n",
    "            except:\n",
    "                print \"Invalid ID field. Continuing.\"\n",
    "                SKIPPED_FILES.append(name)\n",
    "            try:\n",
    "                CLASSIFICATION = str(m_soup.select_one(\".field-name-field-original-classification\").select_one(\".field-item\").string or \"\")\n",
    "            except:\n",
    "                print \"Invalid CLASSIFICATION field. Continuing.\"\n",
    "                SKIPPED_FILES.append(name)\n",
    "                \n",
    "            if download_bool:\n",
    "                PDF = m_soup.select_one(\".file\").select_one(\"a\").get('href')\n",
    "                retrieve_file(PDF, ID)\n",
    "            \n",
    "            df.loc[idx] = [ID, TITLE, CLASSIFICATION, PUB_DATE]\n",
    "            idx+=1  \n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:37<00:09,  4.72s/it]"
     ]
    }
   ],
   "source": [
    "sciab = sciab_df(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sciab[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
